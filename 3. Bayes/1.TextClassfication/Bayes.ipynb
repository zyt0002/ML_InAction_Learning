{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建数据集：词条和标签向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    # 切分的词条\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    # 类别标签向量，1代表侮辱性词汇，0代表不是\n",
    "    classVec = [0, 1, 0, 1, 0, 1]\n",
    "    # 返回实验样本切分的词条、类别标签向量\n",
    "    return postingList, classVec # 返回 切分词条后（去标点）的文档组合和 类别标签的集合"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将样本词条整理为不重复单词的词条列表：词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) # 求集合并集\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将词条转换为向量：\n",
    "词集模型 AND 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList) # 元素全为0的向量\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec [vocabList.index(word)]= 1\n",
    "        else:\n",
    "            print(\"the word : %s is not in vocabList\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagofWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList) # 元素全为0的向量\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec [vocabList.index(word)] += 1 # 统计出现次数\n",
    "        else:\n",
    "            print(\"the word : %s is not in vocabList\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用词汇表检查向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listOPosts, listClasses = loadDataSet()\n",
    "# myVocabList = createVocabList(listOPosts)\n",
    "# print(setOfWords2Vec(myVocabList, listOPosts[0])) # listOPosts[0] 代表第一行向量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练算法：通过词向量计算概率\n",
    "\n",
    "        改进：降低特殊情况的影响，所有词出现次数初始化为1，分母初始化为2，取对数防止下溢出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    # 计算训练文档数目\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # 计算每篇文档的词条数目\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 文档属于侮辱类的概率\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    # 创建numpy.zeros数组，词条出现数初始化为0\n",
    "    # p0Num = np.zeros(numWords)\n",
    "    # p1Num = np.zeros(numWords)\n",
    "    # 创建numpy.ones数组，词条出现数初始化为1,拉普拉斯平滑\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)\n",
    "    # 分母初始化为0\n",
    "    # p0Denom = 0.0\n",
    "    # p1Denom = 0.0\n",
    "    # 分母初始化为2，拉普拉斯平滑\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        # 统计属于侮辱类的条件概率所需的数据，即P(w0|1),P(w1|1),P(w2|1)...\n",
    "        if trainCategory[i] == 1:\n",
    "            # 统计所有侮辱类文档中每个单词出现的个数\n",
    "            p1Num += trainMatrix[i]\n",
    "            # 统计一共出现的侮辱单词的个数\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        # 统计属于非侮辱类的条件概率所需的数据，即P(w0|0),P(w1|0),P(w2|0)...\n",
    "        else:\n",
    "            # 统计所有非侮辱类文档中每个单词出现的个数\n",
    "            p0Num += trainMatrix[i]\n",
    "            # 统计一共出现的非侮辱单词的个数\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # 每个侮辱类单词分别出现的概率\n",
    "    # p1Vect = p1Num / p1Denom\n",
    "    # 取对数，防止下溢出\n",
    "    p1Vect = np.log(p1Num / p1Denom)\n",
    "    # 每个非侮辱类单词分别出现的概率\n",
    "    # p0Vect = p0Num / p0Denom\n",
    "    # 取对数，防止下溢出\n",
    "    p0Vect = np.log(p0Num / p0Denom)\n",
    "    # 返回属于侮辱类的条件概率数组、属于非侮辱类的条件概率数组、文档属于侮辱类的概率\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listOPosts, listClasses = loadDataSet()\n",
    "# myVocabList = createVocabList(listOPosts)\n",
    "# trainMat = []\n",
    "# for postinDoc in listOPosts: # 填充训练矩阵\n",
    "#     trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "\n",
    "# p0V,p1V,pAb = trainNB0(trainMat, listClasses)\n",
    "# # 矩阵中位置对应词汇表单词出现的概率\n",
    "# print(myVocabList)\n",
    "# print(p0V) \n",
    "# print(p1V)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯分类函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    # 对应元素相乘，logA*B = logA + logB所以这里是累加\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1) # 向量元素依次相乘\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1) \n",
    "    print('p0:', p0)\n",
    "    print('p1:', p1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    # 创建实验样本\n",
    "    listOPosts, listclasses = loadDataSet()\n",
    "    # 创建词汇表,将输入文本中的不重复的单词进行提取组成单词向量\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        # 将实验样本向量化若postinDoc中的单词在myVocabList出现则将returnVec该位置的索引置1\n",
    "        # 将6组数据list存储在trainMat中\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    # 训练朴素贝叶斯分类器\n",
    "    p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listclasses))\n",
    "    # 测试样本1\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    # 测试样本向量化返回这三个单词出现位置的索引\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    if classifyNB(thisDoc, p0V, p1V, pAb):\n",
    "        # 执行分类并打印结果\n",
    "        print(testEntry, '属于侮辱类')\n",
    "    else:\n",
    "        # 执行分类并打印结果\n",
    "        print(testEntry, '属于非侮辱类')\n",
    "    # 测试样本2\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    # 将实验样本向量化\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    if classifyNB(thisDoc, p0V, p1V, pAb):\n",
    "        # 执行分类并打印结果\n",
    "        print(testEntry, '属于侮辱类')\n",
    "    else:\n",
    "        # 执行分类并打印结果\n",
    "        print(testEntry, '属于非侮辱类')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0: -7.694848072384611\n",
      "p1: -9.826714493730215\n",
      "['love', 'my', 'dalmation'] 属于非侮辱类\n",
      "p0: -7.20934025660291\n",
      "p1: -4.702750514326955\n",
      "['stupid', 'garbage'] 属于侮辱类\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
