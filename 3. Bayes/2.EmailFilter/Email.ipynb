{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切分文本：\n",
    "spilt和正则表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    # mySent = 'This is taoshao, From BeiJing University.'\n",
    "    # mySent.split() #标点被当成词的一部分\n",
    "    # regEx = re.compile('\\\\W*') # W* 0个或多个非字母数字或下划线字符（等价于[^a-zA-Z0-9_]）\n",
    "    listOfTokens = re.split(r'\\\\W*', bigString)\n",
    "    # 去空白串 + 转为小写，方便词袋处理\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) >2 ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把构建好的朴素贝叶斯代码导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    # 创建一个空的不重复列表\n",
    "    # set是一个无序且不重复的元素集合\n",
    "    vocabSet = set([])\n",
    "    for document in dataSet:\n",
    "        # 取并集\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明：根据vocabList词汇表，将inputSet向量化，向量的每个元素为1或0\n",
    "\n",
    "Parameters:\n",
    "    vocabList - createVocabList返回的列表\n",
    "    inputSet - 切分的词条列表\n",
    "    \n",
    "Returns:\n",
    "    returnVec - 文档向量，词集模型\n",
    "\n",
    "Modify:\n",
    "    2018-07-21\n",
    "\"\"\"\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    # 创建一个其中所含元素都为0的向量\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    # 遍历每个词条\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            # 如果词条存在于词汇表中，则置1\n",
    "            # index返回word出现在vocabList中的索引\n",
    "            # 若这里改为+=则就是基于词袋的模型，遇到一个单词会增加单词向量中德对应值\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary\" % word)\n",
    "    # 返回文档向量\n",
    "    return returnVec\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明：根据vocabList词汇表，构造词袋模型\n",
    "\n",
    "Parameters:\n",
    "    vocabList - createVocabList返回的列表\n",
    "    inputSet - 切分的词条列表\n",
    "    \n",
    "Returns:\n",
    "    returnVec - 文档向量，词袋模型\n",
    "\n",
    "Modify:\n",
    "    2018-07-21\n",
    "\"\"\"\n",
    "def bagOfWords2Vec(vocabList, inputSet):\n",
    "    # 创建一个其中所含元素都为0的向量\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    # 遍历每个词条\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            # 如果词条存在于词汇表中，则置1\n",
    "            # index返回word出现在vocabList中的索引\n",
    "            # 若这里改为+=则就是基于词袋的模型，遇到一个单词会增加单词向量中德对应值\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary\" % word)\n",
    "    # 返回文档向量\n",
    "    return returnVec\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "函数说明：朴素贝叶斯分类器训练函数\n",
    "\n",
    "Parameters:\n",
    "    trainMatrix - 训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵\n",
    "    trainCategory - 训练类标签向量，即loadDataSet返回的classVec\n",
    "    \n",
    "Returns:\n",
    "    p0Vect - 侮辱类的条件概率数组\n",
    "    p1Vect - 非侮辱类的条件概率数组\n",
    "    pAbusive - 文档属于侮辱类的概率\n",
    "\n",
    "Modify:\n",
    "    2018-07-21\n",
    "\"\"\"\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    # 计算训练文档数目\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # 计算每篇文档的词条数目\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 文档属于侮辱类的概率\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    # 创建numpy.zeros数组，词条出现数初始化为0\n",
    "    # p0Num = np.zeros(numWords)\n",
    "    # p1Num = np.zeros(numWords)\n",
    "    # 创建numpy.ones数组，词条出现数初始化为1,拉普拉斯平滑\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)\n",
    "    # 分母初始化为0\n",
    "    # p0Denom = 0.0\n",
    "    # p1Denom = 0.0\n",
    "    # 分母初始化为2，拉普拉斯平滑\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        # 统计属于侮辱类的条件概率所需的数据，即P(w0|1),P(w1|1),P(w2|1)...\n",
    "        if trainCategory[i] == 1:\n",
    "            # 统计所有侮辱类文档中每个单词出现的个数\n",
    "            p1Num += trainMatrix[i]\n",
    "            # 统计一共出现的侮辱单词的个数\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        # 统计属于非侮辱类的条件概率所需的数据，即P(w0|0),P(w1|0),P(w2|0)...\n",
    "        else:\n",
    "            # 统计所有非侮辱类文档中每个单词出现的个数\n",
    "            p0Num += trainMatrix[i]\n",
    "            # 统计一共出现的非侮辱单词的个数\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # 每个侮辱类单词分别出现的概率\n",
    "    # p1Vect = p1Num / p1Denom\n",
    "    # 取对数，防止下溢出\n",
    "    p1Vect = np.log(p1Num / p1Denom)\n",
    "    # 每个非侮辱类单词分别出现的概率\n",
    "    # p0Vect = p0Num / p0Denom\n",
    "    # 取对数，防止下溢出\n",
    "    p0Vect = np.log(p0Num / p0Denom)\n",
    "    # 返回属于侮辱类的条件概率数组、属于非侮辱类的条件概率数组、文档属于侮辱类的概率\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "\n",
    "\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    # 对应元素相乘\n",
    "    # p1 = reduce(lambda x,y:x*y, vec2Classify * p1Vec) * pClass1\n",
    "    # p0 = reduce(lambda x,y:x*y, vec2Classify * p0Vec) * (1.0 - pClass1)\n",
    "    # 对应元素相乘，logA*B = logA + logB所以这里是累加\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    # print('p0:', p0)\n",
    "    # print('p1:', p1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯+交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    # 遍历25个txt文件\n",
    "    for i in range(1, 26):\n",
    "        # 读取每个垃圾邮件，并以字符串转换成字符串列表\n",
    "        wordList = textParse(open('email/spam/%d.txt' % i, 'rb').read().decode('utf8','ignore'))\n",
    "        docList.append(wordList)\n",
    "        fullText.append(wordList)\n",
    "        # 标记垃圾邮件，1表示垃圾文件\n",
    "        classList.append(1)\n",
    "        # 读取每个非垃圾邮件，并以字符串转换成字符串列表\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i, 'rb').read().decode('utf8','ignore'))\n",
    "        docList.append(wordList)\n",
    "        fullText.append(wordList)\n",
    "        # 标记非垃圾邮件，0表示非垃圾文件\n",
    "        classList.append(0)\n",
    "    # 创建词汇表，不重复\n",
    "    vocabList = createVocabList(docList)\n",
    "    # 创建存储训练集的索引值的列表和测试集的索引值的列表\n",
    "    trainingSet = list(range(50))\n",
    "    testSet = []\n",
    "    # 从50个邮件中，随机挑选出40个作为训练集，10个作为测试集\n",
    "    for i in range(10):\n",
    "        # 随机选取索引值,随机生成一个实数\n",
    "        randIndex = int(random.uniform(0, len(trainingSet)))\n",
    "        # 添加测试集的索引值\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        # 在训练集列表中删除添加到测试集的索引值\n",
    "        del(trainingSet[randIndex])\n",
    "    # 创建训练集矩阵和训练集类别标签向量\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    # 遍历训练集\n",
    "    for docIndex in trainingSet:\n",
    "        # 将生成的词集模型添加到训练集矩阵中\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        # 将类别添加到训练集类别标签向量中\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    # 训练朴素贝叶斯模型\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    # 错误分类计数\n",
    "    errorCount = 0\n",
    "    # 遍历测试集\n",
    "    for docIndex in testSet:\n",
    "        # 测试集的词集模型\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        # 如果分类错误\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            # 错误计数器加1\n",
    "            errorCount += 1\n",
    "            print(\"分类错误的测试集：\", docList[docIndex])\n",
    "    print(\"错误率：%.2f%%\" % (float(errorCount) / len(testSet) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类错误的测试集： ['ordercializviagra online & save 75-90%\\r\\n\\r\\n0nline pharmacy noprescription required\\r\\nbuy canadian drugs at wholesale prices and save 75-90%\\r\\nfda-approved drugs + superb quality drugs only!\\r\\naccept all major credit cards\\r\\n        order today! from $1.38\\r\\n']\n",
      "分类错误的测试集： ['hydrocodone/vicodin es/brand watson\\r\\n\\r\\nvicodin es - 7.5/750 mg: 30 - $195 / 120 $570\\r\\nbrand watson - 7.5/750 mg: 30 - $195 / 120 $570\\r\\nbrand watson - 10/325 mg: 30 - $199 / 120 - $588\\r\\nnoprescription required\\r\\nfree express fedex (3-5 days delivery) for over $200 order\\r\\nmajor credit cards + e-check']\n",
      "错误率：20.00%\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
